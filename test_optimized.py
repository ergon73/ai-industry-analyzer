# test_optimized.py - –¢–µ—Å—Ç–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞

import os
import feedparser
import requests
from bs4 import BeautifulSoup
from crewai import Agent, Task, Crew, Process
from crewai_tools import ScrapeWebsiteTool
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# –ó–∞–≥—Ä—É–∂–∞–µ–º API –∫–ª—é—á–∏
load_dotenv()

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏
smart_llm = ChatOpenAI(model_name="gpt-4.1", temperature=0.1)
fast_llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ RSS
def collect_articles_from_rss(rss_feed_urls: str) -> str:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ RSS-–ª–µ–Ω—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
    all_articles_content = ""
    urls = [url.strip() for url in rss_feed_urls.split(',')]

    print(f"--- [–§–£–ù–ö–¶–ò–Ø] –ù–∞—á–∏–Ω–∞—é —Ä–∞–±–æ—Ç—É —Å {len(urls)} RSS-–ª–µ–Ω—Ç–∞–º–∏ ---")

    for url in urls:
        try:
            print(f"üì° –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é RSS-–ª–µ–Ω—Ç—É: {url}")
            feed = feedparser.parse(url)
            
            if not feed.entries:
                print(f"‚ö†Ô∏è  RSS-–ª–µ–Ω—Ç–∞ {url} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç–∞—Ç–µ–π")
                continue
                
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 2 –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
            for entry in feed.entries[:2]:
                print(f"üìÑ –°–∫–∞—á–∏–≤–∞—é —Å—Ç–∞—Ç—å—é: {entry.title} ---")
                try:
                    response = requests.get(entry.link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏
                    for script in soup(["script", "style", "nav", "footer", "header"]):
                        script.decompose()

                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
                    content = soup.find('article') or soup.find('div', class_='content') or soup.find('main') or soup.find('body')
                    if content:
                        text = content.get_text(separator='\n', strip=True)
                        all_articles_content += f"\n\n--- –°–¢–ê–¢–¨–Ø: {entry.title} ---\n{text}"
                    else:
                        all_articles_content += f"\n\n--- –°–¢–ê–¢–¨–Ø: {entry.title} ---\n–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç."

                except Exception as e:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Å—Ç–∞—Ç—å—é {entry.link}: {e}")
                    continue
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ RSS-–ª–µ–Ω—Ç—ã {url}: {e}")
            continue

    if not all_articles_content.strip():
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç–µ–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å RSS-–ª–µ–Ω—Ç."
        
    return all_articles_content

# –°–æ–∑–¥–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
scraper_tool = ScrapeWebsiteTool()

# –ê–≥–µ–Ω—Ç—ã
news_analyst = Agent(
    role='–í–µ–¥—É—â–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Å—Ñ–µ—Ä–µ –ò–ò',
    goal="–°–æ–±—Ä–∞—Ç—å —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ RSS-–ª–µ–Ω—Ç.",
    backstory="–í—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π IT-–∂—É—Ä–Ω–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –ø—É–ª—å—Å –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –ò–ò.",
    tools=[scraper_tool],
    llm=fast_llm,
    verbose=True
)

trend_analyst = Agent(
    role='–ê–Ω–∞–ª–∏—Ç–∏–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤',
    goal="–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã —Å—Ç–∞—Ç–µ–π –∏ –≤—ã–¥–µ–ª–∏—Ç—å 5 –∫–ª—é—á–µ–≤—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤.",
    backstory="–í—ã ‚Äî data-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º —Ä—ã–Ω–∫–∞ –ò–ò.",
    llm=smart_llm,
    verbose=True
)

critical_analyst = Agent(
    role='–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∏ —Å–∫–µ–ø—Ç–∏–∫',
    goal="–ù–∞–π—Ç–∏ –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ —Å–≤—è–∑–∏, —Ä–∏—Å–∫–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤.",
    backstory="–í—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –≤–µ–Ω—á—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤–∏–¥–∞–ª —Å–æ—Ç–Ω–∏ '—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö' —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π.",
    llm=smart_llm,
    verbose=True
)

strategic_reviewer = Agent(
    role='–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –æ–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å',
    goal="–°–æ–∑–¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç, –æ–±—ä–µ–¥–∏–Ω–∏–≤ –∞–Ω–∞–ª–∏–∑ –∏ –∫—Ä–∏—Ç–∏–∫—É.",
    backstory="–í—ã ‚Äî '–ø—Ä–∞–≤–∞—è —Ä—É–∫–∞' —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞.",
    llm=fast_llm,
    verbose=True
)

# RSS-–ª–µ–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞
RSS_FEEDS = """
    https://www.theverge.com/rss/ai-artificial-intelligence/index.xml,
    https://techcrunch.com/category/artificial-intelligence/feed/
"""

# –ó–∞–¥–∞—á–∏
task_fetch_and_parse = Task(
    description=f"""–°–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç—å–∏ –∏–∑ RSS-–ª–µ–Ω—Ç: {RSS_FEEDS}
    
    –ò—Å–ø–æ–ª—å–∑—É–π —Ñ—É–Ω–∫—Ü–∏—é collect_articles_from_rss –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç–µ–π.
    –°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Å—Ç–∞—Ç—å—è—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ò–ò, –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏.""",
    expected_output="–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π.",
    agent=news_analyst
)

task_analyze_trends = Task(
    description="–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã —Å—Ç–∞—Ç–µ–π –∏ –≤—ã–¥–µ–ª–∏—Ç—å 5 –≥–ª–∞–≤–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤.",
    expected_output="–°–ø–∏—Å–æ–∫ 5 —Ç—Ä–µ–Ω–¥–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.",
    agent=trend_analyst,
    context=[task_fetch_and_parse]
)

task_critique_trends = Task(
    description="""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:
    1. –ö–∞–∫–æ–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –ü–†–Ø–ú–û –°–ï–ô–ß–ê–°?
    2. –ö–∞–∫–æ–π —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —Ä–∏—Å–∫?
    3. –ö—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–≥—Ä–∞—Ç—å, –∞ –∫—Ç–æ –≤—ã–∏–≥—Ä–∞—Ç—å?""",
    expected_output="–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞.",
    agent=critical_analyst,
    context=[task_analyze_trends]
)

task_generate_report_final = Task(
    description="–°–æ–∑–¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç, –æ–±—ä–µ–¥–∏–Ω–∏–≤ –∞–Ω–∞–ª–∏–∑ –∏ –∫—Ä–∏—Ç–∏–∫—É.",
    expected_output="–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.",
    agent=strategic_reviewer,
    context=[task_analyze_trends, task_critique_trends]
)

# Crew
ai_news_crew = Crew(
    agents=[news_analyst, trend_analyst, critical_analyst, strategic_reviewer],
    tasks=[task_fetch_and_parse, task_analyze_trends, task_critique_trends, task_generate_report_final],
    process=Process.sequential,
    verbose=True
)

# –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    try:
        print("üöÄ –ó–∞–ø—É—Å–∫–∞—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π...")
        result = ai_news_crew.kickoff()

        print("\n\n##################################")
        print("## –ì–û–¢–û–í–´–ô –ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ß–ï–¢ ##")
        print("##################################\n")
        print(result)
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}") 